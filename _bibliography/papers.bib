---
---

@string{aps = {American Physical Society,}}

@inproceedings{Lu23CausalState,
  title = {Causal {{State Distillation}} for {{Explainable Reinforcement Learning}}},
  author = {Lu, Wenhao and Zhao, Xufeng and Fryen, Thilo and Lee, Jae Hee and Li, Mengdi and Magg, Sven and Wermter, Stefan},
  booktitle = {3rd Conference on Causal Learning and Reasoning (<b>CLeaR 2024</b>) (<p style="color:red;display:inline"><b>oral</b></p>), Los Angeles, California, USA. <p style="color:orange;display:inline">[Scheduled]</p>},
  year = {2024},
  month = April,
  number = {arXiv:2401.00104},
  arxiv = {2401.00104},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2401.00104},
  urldate = {2024-01-02},
  abstract = {Reinforcement learning (RL) is a powerful technique for training intelligent agents, but understanding why these agents make specific decisions can be quite challenging. This lack of transparency in RL models has been a long-standing problem, making it difficult for users to grasp the reasons behind an agent's behaviour. Various approaches have been explored to address this problem, with one promising avenue being reward decomposition (RD). RD is appealing as it sidesteps some of the concerns associated with other methods that attempt to rationalize an agent's behaviour in a post-hoc manner. RD works by exposing various facets of the rewards that contribute to the agent's objectives during training. However, RD alone has limitations as it primarily offers insights based on sub-rewards and does not delve into the intricate cause-and-effect relationships that occur within an RL agent's neural model. In this paper, we present an extension of RD that goes beyond sub-rewards to provide more informative explanations. Our approach is centred on a causal learning framework that leverages information-theoretic measures for explanation objectives that encourage three crucial properties of causal factors: {\textbackslash}emph\{causal sufficiency\}, {\textbackslash}emph\{sparseness\}, and {\textbackslash}emph\{orthogonality\}. These properties help us distill the cause-and-effect relationships between the agent's states and actions or rewards, allowing for a deeper understanding of its decision-making processes. Our framework is designed to generate local explanations and can be applied to a wide range of RL tasks with multiple reward channels. Through a series of experiments, we demonstrate that our approach offers more meaningful and insightful explanations for the agent's action selections.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Methodology},
  pages = {2401.00104},
  preview= {RD_flow_overview.jpg},
  journaltitle = {}
}

@inproceedings{Lu23CloserLook,
  title = {A {{Closer Look}} at {{Reward Decomposition}} for {{High-Level Robotic Explanations}}},
  author = {Lu, Wenhao and Zhao, Xufeng and Magg, Sven and Gromniak, Martin and Wermter, Stefan},
  booktitle = {IEEE International Conference on Development and Learning (<b>ICDL 2023</b>), Macau, China},
  year = {2023},
  month = Nov,
  number = {arXiv:2304.12958},
  eprint = {2304.12958},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2304.12958},
  urldate = {2023-08-21},
  abstract = {Explaining the behavior of intelligent agents such as robots to humans is challenging due to their incomprehensible proprioceptive states, variational intermediate goals, and resultant unpredictability. Moreover, one-step explanations for reinforcement learning agents can be ambiguous as they fail to account for the agent's future behavior at each transition, adding to the complexity of explaining robot actions. By leveraging abstracted actions that map to task-specific primitives, we avoid explanations on the movement level. Our proposed framework combines reward decomposition (RD) with abstracted action spaces into an explainable learning framework, allowing for non-ambiguous and high-level explanations based on object properties in the task. We demonstrate the effectiveness of our framework through quantitative and qualitative analysis of two robot scenarios, showcasing visual and textual explanations, from output artifacts of RD explanation, that are easy for humans to comprehend. Additionally, we demonstrate the versatility of integrating these artifacts with large language models for reasoning and interactive querying.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  arxiv = {2304.12958},
  abbr = {IEEE ICDL 2023},
}
